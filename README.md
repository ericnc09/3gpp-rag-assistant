# 3GPP Technical Specification RAG Assistant ğŸš€

An AI-powered Retrieval-Augmented Generation (RAG) system that makes navigating 3GPP technical specifications effortless. Ask questions in natural language and get accurate, cited answers from 5G/LTE documentation.

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## ğŸ¯ Project Overview

During my work on 5G network deployments at Rogers Communications, I noticed engineers spending hours searching through 3GPP specifications. This RAG system automates that process using:

- **Vector embeddings** for semantic search across technical documents
- **Large Language Models** for natural language understanding
- **Citation tracking** to ensure answer accuracy and traceability
- **Production-ready architecture** with FastAPI backend and monitoring

## âœ¨ Features

- ğŸ” **Semantic Search**: Find relevant information across 1000+ pages of specs using local embeddings
- ğŸ’¬ **Natural Language Queries**: Ask questions like "What is the gNB architecture?"
- ğŸ“š **Citation Tracking**: Every answer includes source document references
- ğŸ’° **No API Costs**: Fully local embeddings - no external API dependencies or costs
- ğŸ”„ **Conversation Memory**: Multi-turn conversations with context retention
- ğŸ“Š **Analytics Dashboard**: Query patterns, performance metrics, and usage stats
- ğŸš€ **Open Source Models**: Built entirely on open source embedding models (sentence-transformers)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â”€â–¶â”‚  FastAPI     â”‚â”€â”€â”€â”€â”€â–¶â”‚  Vector DB  â”‚
â”‚  (Streamlit)â”‚      â”‚   Backend    â”‚      â”‚  (Chroma)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Local LLM    â”‚
                     â”‚ (Open Source)â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Embeddings**: Local models (sentence-transformers) - no API required âœ¨

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9 or higher
- Git
- (Optional) CUDA/GPU for faster embedding generation

### Installation

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/3gpp-rag-assistant.git
cd 3gpp-rag-assistant

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# (Optional) Set up environment variables for advanced configuration
cp .env.example .env
```

**Note**: No API keys required! All embeddings are generated locally using open source models.

### Running the Application

```bash
# 1. Download and process 3GPP specifications
python src/core/document_processor.py

# 2. Start the FastAPI backend
uvicorn src.api.main:app --reload --port 8000

# 3. Launch the Streamlit frontend (in a new terminal)
streamlit run src/frontend/app.py
```

Visit `http://localhost:8501` to start asking questions!

## ğŸ“– Example Queries

Try asking:
- "What is the difference between SA and NSA 5G deployment?"
- "Explain the NG-RAN architecture"
- "What are the key features of NR physical layer?"
- "How does handover work in 5G networks?"

## ğŸ“ Project Structure

```
3gpp-rag-assistant/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/              # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ main.py       # API endpoints
â”‚   â”‚   â””â”€â”€ models.py     # Pydantic schemas
â”‚   â”œâ”€â”€ core/             # Core RAG logic
â”‚   â”‚   â”œâ”€â”€ embeddings.py # Vector embedding generation
â”‚   â”‚   â”œâ”€â”€ retriever.py  # Document retrieval
â”‚   â”‚   â”œâ”€â”€ llm.py        # LLM interaction
â”‚   â”‚   â””â”€â”€ document_processor.py  # PDF parsing
â”‚   â”œâ”€â”€ utils/            # Utility functions
â”‚   â”‚   â”œâ”€â”€ logger.py     # Logging configuration
â”‚   â”‚   â””â”€â”€ metrics.py    # Cost and performance tracking
â”‚   â””â”€â”€ frontend/         # Streamlit UI
â”‚       â””â”€â”€ app.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Original 3GPP PDFs
â”‚   â””â”€â”€ processed/        # Parsed and chunked documents
â”œâ”€â”€ tests/                # Unit and integration tests
â”œâ”€â”€ notebooks/            # Jupyter notebooks for exploration
â”œâ”€â”€ docs/                 # Additional documentation
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env.example          # Environment variables template
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

Key configuration options in `.env`:

```env
# Embedding Configuration (Local Models - No API Key Needed!)
# Options: "mini" (fast), "mpnet" (better), "bge-small" (technical docs), "bge-base" (best quality)
EMBEDDING_MODEL=mini

# Vector Database
VECTOR_DB_PATH=./data/vectordb
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# API Configuration
API_PORT=8000
LOG_LEVEL=INFO
```

### Available Embedding Models

All models are **completely free** and run locally:

| Model | Size | Dimensions | Speed | Best For |
|-------|------|-----------|-------|----------|
| `mini` | 80MB | 384 | âš¡ Fast | Quick prototyping |
| `mpnet` | 420MB | 768 | ğŸ”„ Medium | Balanced performance |
| `bge-small` | 130MB | 384 | âš¡ Fast | Technical documents |
| `bge-base` | 440MB | 768 | ğŸš€ Best | Maximum accuracy |

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src tests/

# Run specific test file
pytest tests/test_retriever.py
```

## ğŸ“Š Performance Metrics

Current benchmarks (based on 3GPP TS 38.300 with all-MiniLM-L6-v2 model):
- **Average query time**: ~1.5-2s (CPU), <0.5s (GPU)
- **Retrieval accuracy**: 90%+ (top-5 chunks contain answer)
- **Cost per query**: **Free** (completely local) ğŸ‰
- **Document processing**: ~50-100 PDFs/minute (CPU)
- **Memory footprint**: ~200MB (model + overhead)

## ğŸ›£ï¸ Roadmap

- [ ] Phase 1: Core RAG functionality with 3GPP specs âœ…
- [ ] Phase 2: Add evaluation metrics (answer quality scoring)
- [ ] Phase 3: Support for multiple document formats (Word, HTML)
- [ ] Phase 4: Fine-tune embedding model on telecom domain
- [ ] Phase 5: Deploy to cloud (AWS/GCP)
- [ ] Phase 6: Multi-user authentication and query history

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **3GPP** for providing open access to technical specifications
- **Sentence Transformers** for excellent open source embedding models
- **Hugging Face** for model hosting and community
- **LangChain** community for RAG best practices
- **Rogers Communications** for inspiring this project
- Open source community for making AI accessible to everyone ğŸ’™

## ğŸ“§ Contact

**Eric Costa**  
Email: ericcosta.public@gmail.com  
LinkedIn: [linkedin.com/in/niloyericcosta](https://linkedin.com/in/niloyericcosta)

---

**Note**: This is a portfolio project demonstrating AI product engineering skills. It is not affiliated with or endorsed by 3GPP or Rogers Communications.
